{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b6351c07",
   "metadata": {},
   "source": [
    "# Dataruns Basic Usage Examples\n",
    "\n",
    "This notebook demonstrates the basic functionality of the dataruns library including:\n",
    "- Data extraction from CSV files\n",
    "- Building simple pipelines\n",
    "- Basic data transformations\n",
    "- Integration with pandas DataFrames\n",
    "\n",
    "## Prerequisites\n",
    "\n",
    "Make sure you have the dataruns library available in your Python environment. The examples use sample data that will be generated in the notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34f98318",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "First, let's import the required libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5052c419",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful!\n",
      "✓ NumPy version: 2.2.5\n",
      "✓ Pandas version: 2.2.3\n",
      "✓ Dataruns version: 1.0.dev\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import dataruns\n",
    "\n",
    "# Import dataruns components\n",
    "from dataruns.core.pipeline import Pipeline, Make_Pipeline\n",
    "from dataruns.source.datasource import CSVSource\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(f\"✓ NumPy version: {np.__version__}\")\n",
    "print(f\"✓ Pandas version: {pd.__version__}\")\n",
    "print(f\"✓ Dataruns version: {dataruns.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c01584",
   "metadata": {},
   "source": [
    "## 1. Basic Pipeline Example\n",
    "\n",
    "Let's start with a simple pipeline that applies a transformation function to data. This demonstrates the core concept of chaining operations in dataruns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8b08ef2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List input: [1, 2, 3, 4, 5]\n",
      "List result: [ 2  4  6  8 10]\n",
      "\n",
      "DataFrame input:\n",
      "   A  B\n",
      "0  1  4\n",
      "1  2  5\n",
      "2  3  6\n",
      "\n",
      "DataFrame result:\n",
      "   A   B\n",
      "0  2   8\n",
      "1  4  10\n",
      "2  6  12\n"
     ]
    }
   ],
   "source": [
    "# Define a simple transformation function\n",
    "def transform_function(data):\n",
    "    \"\"\"Simple transformation that doubles the values.\"\"\"\n",
    "    if isinstance(data, pd.DataFrame):\n",
    "        return data * 2\n",
    "    elif isinstance(data, (list, np.ndarray)):\n",
    "        return np.array(data) * 2\n",
    "    else:\n",
    "        return data * 2\n",
    "\n",
    "# Create pipeline\n",
    "pipeline = Pipeline(transform_function)\n",
    "\n",
    "# Test with list\n",
    "input_list = [1, 2, 3, 4, 5]\n",
    "result_list = pipeline(input_list)\n",
    "print(f\"List input: {input_list}\")\n",
    "print(f\"List result: {result_list}\")\n",
    "\n",
    "# Test with DataFrame\n",
    "df = pd.DataFrame({'A': [1, 2, 3], 'B': [4, 5, 6]})\n",
    "result_df = pipeline(df)\n",
    "print(f\"\\nDataFrame input:\")\n",
    "print(df)\n",
    "print(f\"\\nDataFrame result:\")\n",
    "print(result_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f788672",
   "metadata": {},
   "source": [
    "## 2. Advanced Pipeline with Multiple Functions\n",
    "\n",
    "Now let's create a more complex pipeline that chains multiple transformation functions together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa14fb19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "     A   B    C\n",
      "0  1.0  10  0.1\n",
      "1  2.0  20  0.2\n",
      "2  NaN  30  0.3\n",
      "3  4.0  40  0.4\n",
      "4  5.0  50  0.5\n",
      "\n",
      "Data shape: (5, 3)\n",
      "Missing values per column:\n",
      "A    1\n",
      "B    0\n",
      "C    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Create sample data with missing values\n",
    "np.random.seed(42)\n",
    "df = pd.DataFrame({\n",
    "    'A': [1, 2, None, 4, 5], \n",
    "    'B': [10, 20, 30, 40, 50],\n",
    "    'C': [0.1, 0.2, 0.3, 0.4, 0.5]\n",
    "})\n",
    "\n",
    "print(\"Original data:\")\n",
    "print(df)\n",
    "print(f\"\\nData shape: {df.shape}\")\n",
    "print(f\"Missing values per column:\")\n",
    "print(df.isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a6761bbd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating pipeline with 3 transformation steps:\n",
      "Pipeline created: Pipeline(\n",
      "    Function[clean_data]\n",
      "    Function[normalize_columns]\n",
      "    Function[add_sum_column]\n",
      ")\n",
      "\n",
      "Applying pipeline...\n",
      "  → Cleaning data: 1 missing values found\n",
      "  → Normalizing 3 columns\n",
      "  → Adding sum column\n",
      "\n",
      "Final result:\n",
      "          A         B         C       sum\n",
      "0 -1.095445 -1.095445 -1.095445 -3.286335\n",
      "1 -0.547723 -0.547723 -0.547723 -1.643168\n",
      "3  0.547723  0.547723  0.547723  1.643168\n",
      "4  1.095445  1.095445  1.095445  3.286335\n",
      "Result shape: (4, 4)\n"
     ]
    }
   ],
   "source": [
    "# Define transformation functions\n",
    "def clean_data(df):\n",
    "    \"\"\"Remove missing values.\"\"\"\n",
    "    print(f\"  → Cleaning data: {df.isnull().sum().sum()} missing values found\")\n",
    "    return df.dropna()\n",
    "\n",
    "def normalize_columns(df):\n",
    "    \"\"\"Normalize columns to have mean 0 and std 1.\"\"\"\n",
    "    print(f\"  → Normalizing {len(df.columns)} columns\")\n",
    "    return (df - df.mean()) / df.std()\n",
    "\n",
    "def add_sum_column(df):\n",
    "    \"\"\"Add a column with the sum of all other columns.\"\"\"\n",
    "    print(f\"  → Adding sum column\")\n",
    "    df = df.copy()\n",
    "    df['sum'] = df.sum(axis=1)\n",
    "    return df\n",
    "\n",
    "# Create pipeline with multiple functions\n",
    "print(\"Creating pipeline with 3 transformation steps:\")\n",
    "pipeline = Pipeline(clean_data, normalize_columns, add_sum_column)\n",
    "print(f\"Pipeline created: {pipeline}\")\n",
    "\n",
    "# Apply the pipeline\n",
    "print(\"\\nApplying pipeline...\")\n",
    "result = pipeline(df)\n",
    "\n",
    "print(f\"\\nFinal result:\")\n",
    "print(result)\n",
    "print(f\"Result shape: {result.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98341fe6",
   "metadata": {},
   "source": [
    "## 3. Pipeline Builder Example\n",
    "\n",
    "The `Make_Pipeline` class allows you to build pipelines step by step, which is useful for dynamic pipeline construction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cb2f41a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original data:\n",
      "   x   y\n",
      "0  1  10\n",
      "1  2  20\n",
      "2  3  30\n",
      "3  4  40\n",
      "4  5  50\n",
      "\n",
      "Building pipeline step by step:\n",
      "  → Added: Double values\n",
      "  → Added: Add 10\n",
      "  → Added: Round to 2 decimals\n",
      "\n",
      "Built pipeline: Pipeline(\n",
      "    Function[<lambda>]\n",
      "    Function[<lambda>]\n",
      "    Function[<lambda>]\n",
      ")\n",
      "\n",
      "Final result:\n",
      "    x    y\n",
      "0  12   30\n",
      "1  14   50\n",
      "2  16   70\n",
      "3  18   90\n",
      "4  20  110\n"
     ]
    }
   ],
   "source": [
    "# Create data for the builder example\n",
    "data = pd.DataFrame({\n",
    "    'x': [1, 2, 3, 4, 5],\n",
    "    'y': [10, 20, 30, 40, 50]\n",
    "})\n",
    "print(\"Original data:\")\n",
    "print(data)\n",
    "\n",
    "# Build pipeline using Make_Pipeline\n",
    "print(\"\\nBuilding pipeline step by step:\")\n",
    "builder = Make_Pipeline()\n",
    "\n",
    "# Add transformations one by one\n",
    "builder.add(lambda df: df * 2)  # Double values\n",
    "print(\"  → Added: Double values\")\n",
    "\n",
    "builder.add(lambda df: df + 10)  # Add 10\n",
    "print(\"  → Added: Add 10\")\n",
    "\n",
    "builder.add(lambda df: df.round(2))  # Round to 2 decimal places\n",
    "print(\"  → Added: Round to 2 decimals\")\n",
    "\n",
    "# Build and apply pipeline\n",
    "pipeline = builder.build()\n",
    "print(f\"\\nBuilt pipeline: {pipeline}\")\n",
    "\n",
    "result = pipeline(data)\n",
    "print(f\"\\nFinal result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "182f656e",
   "metadata": {},
   "source": [
    "## 4. CSV Data Extraction Example\n",
    "\n",
    "Let's create a sample CSV file and demonstrate how to extract data using the CSVSource class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "57d74d7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Created sample CSV file: sample_data.csv\n",
      "Sample data shape: (20, 4)\n",
      "\n",
      "First 5 rows of sample data:\n",
      "   feature1  feature2  target category\n",
      "0  0.496714  7.931298       0        A\n",
      "1 -0.138264  4.548447       1        B\n",
      "2  0.647689  5.135056       1        C\n",
      "3  1.523030  2.150504       1        A\n",
      "4 -0.234153  3.911235       1        B\n"
     ]
    }
   ],
   "source": [
    "# Create sample CSV data\n",
    "sample_data = pd.DataFrame({\n",
    "    'feature1': np.random.normal(0, 1, 20),\n",
    "    'feature2': np.random.normal(5, 2, 20),\n",
    "    'target': np.random.randint(0, 2, 20),\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 20)\n",
    "})\n",
    "\n",
    "# Save to CSV file\n",
    "csv_path = 'sample_data.csv'\n",
    "sample_data.to_csv(csv_path, index=False)\n",
    "print(f\"✓ Created sample CSV file: {csv_path}\")\n",
    "print(f\"Sample data shape: {sample_data.shape}\")\n",
    "print(\"\\nFirst 5 rows of sample data:\")\n",
    "print(sample_data.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "695d11bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Successfully extracted data using CSVSource\n",
      "Extracted data shape: (20, 4)\n",
      "\n",
      "First 5 rows of extracted data:\n",
      "               feature1            feature2 target category\n",
      "0    0.4967141530112327   7.931297537843108      0        A\n",
      "1  -0.13826430117118466   4.548447399026928      1        B\n",
      "2    0.6476885381006925  5.1350564093758475      1        C\n",
      "3    1.5230298564080254  2.1505036275730864      1        A\n",
      "4  -0.23415337472333597  3.9112345509496347      1        B\n",
      "  → Selecting numeric columns...\n",
      "  → Found 0 numeric columns: []\n",
      "  → Filling missing values with mean...\n",
      "\n",
      "✓ Pipeline processing completed\n",
      "Processed data shape: (20, 0)\n",
      "\n",
      "Processed data (first 5 rows):\n",
      "Empty DataFrame\n",
      "Columns: []\n",
      "Index: [0, 1, 2, 3, 4]\n",
      "\n",
      "✓ Cleaned up: Removed sample_data.csv\n"
     ]
    }
   ],
   "source": [
    "# Extract data using CSVSource\n",
    "try:\n",
    "    source = CSVSource(file_path=csv_path)\n",
    "    extracted_data = source.extract_data()\n",
    "    print(f\"✓ Successfully extracted data using CSVSource\")\n",
    "    print(f\"Extracted data shape: {extracted_data.shape}\")\n",
    "    print(\"\\nFirst 5 rows of extracted data:\")\n",
    "    print(extracted_data.head())\n",
    "    \n",
    "    # Apply a simple pipeline to the CSV data\n",
    "    def process_csv_data(df):\n",
    "        \"\"\"Process CSV data by selecting numeric columns and filling missing values.\"\"\"\n",
    "        print(\"  → Selecting numeric columns...\")\n",
    "        numeric_df = df.select_dtypes(include=[np.number])\n",
    "        print(f\"  → Found {len(numeric_df.columns)} numeric columns: {list(numeric_df.columns)}\")\n",
    "        print(\"  → Filling missing values with mean...\")\n",
    "        return numeric_df.fillna(numeric_df.mean())\n",
    "    \n",
    "    # Create and apply pipeline\n",
    "    pipeline = Pipeline(process_csv_data)\n",
    "    processed_data = pipeline(extracted_data)\n",
    "    \n",
    "    print(f\"\\n✓ Pipeline processing completed\")\n",
    "    print(f\"Processed data shape: {processed_data.shape}\")\n",
    "    print(\"\\nProcessed data (first 5 rows):\")\n",
    "    print(processed_data.head())\n",
    "    \n",
    "    # Clean up - remove the sample CSV file\n",
    "    os.remove(csv_path)\n",
    "    print(f\"\\n✓ Cleaned up: Removed {csv_path}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"✗ Error: {e}\")\n",
    "    # Clean up even if there's an error\n",
    "    if os.path.exists(csv_path):\n",
    "        os.remove(csv_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6a0d7e4",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook demonstrated the core functionality of the dataruns library:\n",
    "\n",
    "1. **Basic Pipeline**: Simple transformation functions chained together\n",
    "2. **Advanced Pipeline**: Multiple transformation steps with data cleaning and normalization\n",
    "3. **Pipeline Builder**: Dynamic pipeline construction using `Make_Pipeline`\n",
    "4. **CSV Extraction**: Data extraction from CSV files using `CSVSource`\n",
    "\n",
    "### Key Takeaways:\n",
    "- ✅ Dataruns provides a clean, functional approach to data processing\n",
    "- ✅ Pipelines can be easily chained and composed\n",
    "- ✅ The library works seamlessly with pandas DataFrames and numpy arrays\n",
    "- ✅ CSV data extraction is straightforward and integrates well with pipeline processing\n",
    "\n",
    "### Next Steps:\n",
    "- Explore the transform examples notebook for advanced data transformation techniques\n",
    "- Check out the comprehensive examples for more complex use cases\n",
    "- Review the documentation for additional features and capabilities"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192c06ef",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataruns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
