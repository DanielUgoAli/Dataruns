{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c48e166",
   "metadata": {},
   "source": [
    "# Dataruns Transform Examples\n",
    "\n",
    "This notebook demonstrates the comprehensive data transformation capabilities of the dataruns library including:\n",
    "\n",
    "- **Scaling Transforms**: StandardScaler, MinMaxScaler\n",
    "- **Missing Value Handling**: DropNA, FillNA\n",
    "- **Column Operations**: SelectColumns, RenameColumns\n",
    "- **Custom Transforms**: ApplyFunction, FilterRows\n",
    "- **Transform Composition**: TransformComposer and preprocessing pipelines\n",
    "- **Integration**: Using transforms with dataruns Pipeline\n",
    "\n",
    "## Overview\n",
    "\n",
    "The dataruns transform system provides a sklearn-like interface for data preprocessing with the flexibility to work with both pandas DataFrames and numpy arrays."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a710c4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All transforms imported successfully!\n",
      "✓ Random seed set for reproducible results\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "# Import dataruns transforms\n",
    "from dataruns.core.transforms import (\n",
    "    StandardScaler, MinMaxScaler, DropNA, FillNA, \n",
    "    SelectColumns, TransformComposer,\n",
    "    create_preprocessing_pipeline\n",
    ")\n",
    "from dataruns.core.pipeline import Pipeline, Make_Pipeline\n",
    "\n",
    "print(\"✓ All transforms imported successfully!\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)\n",
    "print(\"✓ Random seed set for reproducible results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf374d7a",
   "metadata": {},
   "source": [
    "## Sample Data Creation\n",
    "\n",
    "Let's create a realistic dataset with different types of features and some missing values to demonstrate the various transforms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "97797d90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Sample data created\n",
      "Data shape: (100, 5)\n",
      "Data types:\n",
      "feature1    float64\n",
      "feature2    float64\n",
      "feature3    float64\n",
      "feature4    float64\n",
      "category     object\n",
      "dtype: object\n",
      "\n",
      "Missing values per column:\n",
      "feature1    5\n",
      "feature2    5\n",
      "feature3    5\n",
      "feature4    0\n",
      "category    0\n",
      "dtype: int64\n",
      "\n",
      "First 10 rows:\n",
      "    feature1  feature2  feature3  feature4 category\n",
      "0  10.993428  3.584629  0.178894  0.662668        A\n",
      "1   9.723471  4.579355  0.280392  0.390667        C\n",
      "2  11.295377  4.657285  0.541526  2.777513        B\n",
      "3  13.046060  4.197723  0.526901  3.288418        B\n",
      "4   9.531693  4.838714 -0.688835  9.314010        A\n",
      "5        NaN  5.404051 -0.468913  1.064159        B\n",
      "6        NaN  6.886186  0.257518  0.930488        B\n",
      "7        NaN  5.174578  0.256893  2.995909        B\n",
      "8        NaN  5.257550  0.257524  0.833467        B\n",
      "9        NaN  4.925554  1.926366  5.340276        A\n"
     ]
    }
   ],
   "source": [
    "# Create sample data with different characteristics\n",
    "data = pd.DataFrame({\n",
    "    'feature1': np.random.normal(10, 2, 100),      # Normal distribution, mean=10, std=2\n",
    "    'feature2': np.random.normal(5, 1, 100),       # Normal distribution, mean=5, std=1  \n",
    "    'feature3': np.random.normal(0, 0.5, 100),     # Normal distribution, mean=0, std=0.5\n",
    "    'feature4': np.random.exponential(2, 100),     # Exponential distribution\n",
    "    'category': np.random.choice(['A', 'B', 'C'], 100)  # Categorical feature\n",
    "})\n",
    "\n",
    "# Add some missing values to make it realistic\n",
    "data.iloc[5:10, 0] = np.nan    # Missing values in feature1\n",
    "data.iloc[15:20, 1] = np.nan   # Missing values in feature2\n",
    "data.iloc[25:30, 2] = np.nan   # Missing values in feature3\n",
    "\n",
    "print(\"✓ Sample data created\")\n",
    "print(f\"Data shape: {data.shape}\")\n",
    "print(f\"Data types:\")\n",
    "print(data.dtypes)\n",
    "print(f\"\\nMissing values per column:\")\n",
    "print(data.isnull().sum())\n",
    "print(f\"\\nFirst 10 rows:\")\n",
    "print(data.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0383e8da",
   "metadata": {},
   "source": [
    "## 1. Individual Transforms\n",
    "\n",
    "Let's start by demonstrating individual transforms and their effects on the data.\n",
    "\n",
    "### Standard Scaling\n",
    "\n",
    "StandardScaler normalizes features to have mean=0 and standard deviation=1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "228dfbb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical features to scale:\n",
      "['feature1', 'feature2', 'feature3', 'feature4']\n",
      "\n",
      "✓ StandardScaler applied\n",
      "Original data shape: (100, 4)\n",
      "Scaled data shape: (100, 4)\n",
      "\n",
      "Original data statistics:\n",
      "       feature1  feature2  feature3  feature4\n",
      "count    95.000    95.000    95.000   100.000\n",
      "mean      9.735     5.013     0.034     2.331\n",
      "std       1.815     0.961     0.548     2.283\n",
      "min       4.761     3.081    -1.621     0.022\n",
      "25%       8.753     4.191    -0.326     0.689\n",
      "50%       9.723     5.069     0.057     1.605\n",
      "75%      10.723     5.502     0.361     3.174\n",
      "max      13.705     7.720     1.926     9.914\n",
      "\n",
      "Scaled data statistics:\n",
      "       feature1  feature2  feature3  feature4\n",
      "count    95.000    95.000    95.000   100.000\n",
      "mean      0.000     0.000     0.000     0.000\n",
      "std       1.000     1.000     1.000     1.000\n",
      "min      -2.741    -2.011    -3.018    -1.012\n",
      "25%      -0.541    -0.856    -0.657    -0.719\n",
      "50%      -0.007     0.058     0.042    -0.318\n",
      "75%       0.544     0.509     0.597     0.370\n",
      "max       2.187     2.818     3.451     3.322\n",
      "\n",
      "Scaled data means (should be ~0): [0.0, 0.0, 0.0, 0.0]\n",
      "Scaled data stds (should be ~1): [1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# Select only numerical columns for scaling\n",
    "numerical_data = data.select_dtypes(include=[np.number])\n",
    "print(\"Numerical features to scale:\")\n",
    "print(list(numerical_data.columns))\n",
    "\n",
    "# Apply StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaled_data = scaler.fit_transform(numerical_data)\n",
    "\n",
    "print(f\"\\n✓ StandardScaler applied\")\n",
    "print(f\"Original data shape: {numerical_data.shape}\")\n",
    "print(f\"Scaled data shape: {scaled_data.shape}\")\n",
    "\n",
    "print(f\"\\nOriginal data statistics:\")\n",
    "print(numerical_data.describe().round(3))\n",
    "\n",
    "print(f\"\\nScaled data statistics:\")\n",
    "print(scaled_data.describe().round(3))\n",
    "\n",
    "print(f\"\\nScaled data means (should be ~0): {scaled_data.mean().round(6).tolist()}\")\n",
    "print(f\"Scaled data stds (should be ~1): {scaled_data.std().round(6).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9377d026",
   "metadata": {},
   "source": [
    "### Transform Composition\n",
    "\n",
    "The `TransformComposer` allows you to chain multiple transforms together into a single preprocessing pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f67f1b09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ TransformComposer pipeline created with 3 steps:\n",
      "  1. DropNA() - Remove missing values\n",
      "  2. SelectColumns() - Select numerical features\n",
      "  3. StandardScaler() - Standardize features\n",
      "\n",
      "✓ Transform composition applied\n",
      "Original data shape: (100, 5)\n",
      "Transformed data shape: (85, 4)\n",
      "Missing values after processing: 0\n",
      "\n",
      "Transformed data (first 5 rows):\n",
      "   feature1  feature2  feature3  feature4\n",
      "0  0.629890 -1.462518  0.306503 -0.675677\n",
      "1 -0.054804 -0.420451  0.496970 -0.792411\n",
      "2  0.792685 -0.338811  0.986999  0.231942\n",
      "3  1.736561 -0.820246  0.959555  0.451206\n",
      "4 -0.158201 -0.148747 -1.321830  3.037184\n",
      "\n",
      "Transformed data statistics:\n",
      "       feature1  feature2  feature3  feature4\n",
      "count    85.000    85.000    85.000    85.000\n",
      "mean      0.000    -0.000     0.000     0.000\n",
      "std       1.000     1.000     1.000     1.000\n",
      "min      -2.731    -1.990    -3.070    -0.951\n",
      "25%      -0.493    -0.840    -0.642    -0.792\n",
      "50%       0.017     0.083     0.077    -0.316\n",
      "75%       0.630     0.567     0.698     0.328\n",
      "max       2.092     2.870     2.143     3.295\n"
     ]
    }
   ],
   "source": [
    "# Create a preprocessing pipeline using TransformComposer\n",
    "composer = TransformComposer(\n",
    "    DropNA(),                                    # Remove rows with missing values\n",
    "    SelectColumns(['feature1', 'feature2', 'feature3', 'feature4']),  # Select specific columns\n",
    "    StandardScaler()                             # Standardize the features\n",
    ")\n",
    "\n",
    "print(\"✓ TransformComposer pipeline created with 3 steps:\")\n",
    "print(\"  1. DropNA() - Remove missing values\")\n",
    "print(\"  2. SelectColumns() - Select numerical features\")  \n",
    "print(\"  3. StandardScaler() - Standardize features\")\n",
    "\n",
    "# Apply the composed transformation\n",
    "transformed_data = composer.fit_transform(data)\n",
    "\n",
    "print(f\"\\n✓ Transform composition applied\")\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "print(f\"Transformed data shape: {transformed_data.shape}\")\n",
    "print(f\"Missing values after processing: {transformed_data.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\nTransformed data (first 5 rows):\")\n",
    "print(transformed_data.head())\n",
    "\n",
    "print(f\"\\nTransformed data statistics:\")\n",
    "print(transformed_data.describe().round(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7deafc5",
   "metadata": {},
   "source": [
    "## 2. Integration with Dataruns Pipeline\n",
    "\n",
    "Now let's see how transforms integrate seamlessly with the main dataruns Pipeline system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "22d32786",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Preprocessing pipeline created using create_preprocessing_pipeline()\n",
      "  - Scale method: MinMax scaling\n",
      "  - Missing values: Fill with mean\n",
      "✓ Dataruns Pipeline created with preprocessing function\n",
      "  → Processing 4 numerical columns\n",
      "\n",
      "✓ Pipeline execution completed\n",
      "Pipeline result shape: (100, 4)\n",
      "Missing values in result: 0\n",
      "\n",
      "Pipeline result data range:\n",
      "Min values: [0.0, 0.0, 0.0, 0.0]\n",
      "Max values: [1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "Pipeline result (first 5 rows):\n",
      "   feature1  feature2  feature3  feature4\n",
      "0  0.696879  0.108516  0.507338  0.064788\n",
      "1  0.554890  0.322946  0.535953  0.037291\n",
      "2  0.730639  0.339745  0.609574  0.278584\n",
      "3  0.926376  0.240679  0.605451  0.330234\n",
      "4  0.533448  0.378855  0.262701  0.939380\n"
     ]
    }
   ],
   "source": [
    "# Create preprocessing pipeline using convenience function\n",
    "preprocessing = create_preprocessing_pipeline(\n",
    "    scale_method='minmax',      # Use MinMax scaling instead of standard\n",
    "    handle_missing='fill',      # Fill missing values instead of dropping\n",
    "    fill_value=None            # Use mean for filling (None means auto-select method)\n",
    ")\n",
    "\n",
    "print(\"✓ Preprocessing pipeline created using create_preprocessing_pipeline()\")\n",
    "print(\"  - Scale method: MinMax scaling\")\n",
    "print(\"  - Missing values: Fill with mean\")\n",
    "\n",
    "# Create a function to apply preprocessing to numerical data\n",
    "def preprocess_data(data):\n",
    "    \"\"\"Apply preprocessing to numerical columns only.\"\"\"\n",
    "    numerical_data = data.select_dtypes(include=[np.number])\n",
    "    print(f\"  → Processing {len(numerical_data.columns)} numerical columns\")\n",
    "    return preprocessing.fit_transform(numerical_data)\n",
    "\n",
    "# Integrate with dataruns Pipeline\n",
    "pipeline = Pipeline(preprocess_data)\n",
    "print(\"✓ Dataruns Pipeline created with preprocessing function\")\n",
    "\n",
    "# Apply the pipeline\n",
    "result = pipeline(data)\n",
    "print(f\"\\n✓ Pipeline execution completed\")\n",
    "print(f\"Pipeline result shape: {result.shape}\")\n",
    "print(f\"Missing values in result: {result.isnull().sum().sum()}\")\n",
    "\n",
    "print(f\"\\nPipeline result data range:\")\n",
    "print(f\"Min values: {result.min().round(3).tolist()}\")\n",
    "print(f\"Max values: {result.max().round(3).tolist()}\")\n",
    "\n",
    "print(f\"\\nPipeline result (first 5 rows):\")\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e02ae9",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated the comprehensive transform system in dataruns:\n",
    "\n",
    "### Key Features Covered:\n",
    "1. **Individual Transforms**: StandardScaler for feature normalization\n",
    "2. **Transform Composition**: TransformComposer for chaining multiple transforms\n",
    "3. **Pipeline Integration**: Seamless integration with dataruns Pipeline system\n",
    "4. **Convenience Functions**: create_preprocessing_pipeline() for common workflows\n",
    "\n",
    "### Benefits:\n",
    "- ✅ **Familiar Interface**: Similar to sklearn's transform API\n",
    "- ✅ **Flexible Composition**: Easy to chain and combine transforms\n",
    "- ✅ **Type Support**: Works with both pandas DataFrames and numpy arrays\n",
    "- ✅ **Missing Value Handling**: Built-in strategies for dealing with missing data\n",
    "- ✅ **Pipeline Integration**: Transforms work seamlessly with dataruns pipelines\n",
    "\n",
    "### Available Transforms:\n",
    "- **Scaling**: `StandardScaler`, `MinMaxScaler`\n",
    "- **Missing Values**: `DropNA`, `FillNA` \n",
    "- **Column Operations**: `SelectColumns`, `RenameColumns`\n",
    "- **Custom**: `ApplyFunction`, `FilterRows`\n",
    "- **Encoding**: `OneHotEncoder`\n",
    "- **Composition**: `TransformComposer`\n",
    "\n",
    "### Next Steps:\n",
    "Explore the comprehensive transform examples notebook for more advanced use cases and custom transformation patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98358bb7",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataruns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
