{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "89dc8086",
   "metadata": {},
   "source": [
    "# Dataruns Transform Library - Comprehensive Examples\n",
    "\n",
    "This notebook provides comprehensive examples demonstrating advanced usage of the dataruns transform library. We'll explore:\n",
    "\n",
    "1. **Standalone Transforms** - Using individual transforms\n",
    "2. **Transform Composer** - Chaining multiple transforms together\n",
    "3. **Dataruns Pipeline Integration** - Integrating transforms with the main pipeline\n",
    "4. **Custom Preprocessing Pipelines** - Using convenience functions\n",
    "5. **Advanced Pipelines** - Building complex preprocessing workflows\n",
    "\n",
    "This notebook demonstrates real-world scenarios where you need to preprocess data before analysis or machine learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4730cdd1",
   "metadata": {},
   "source": [
    "## Setup and Imports\n",
    "\n",
    "Let's start by importing the necessary libraries and setting up our environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26c0d208",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ All imports successful!\n",
      "NumPy version: 2.2.5\n",
      "Pandas version: 2.2.3\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from dataruns.core.transforms import (\n",
    "    StandardScaler, MinMaxScaler, DropNA, FillNA, \n",
    "    SelectColumns, TransformComposer,\n",
    "    create_preprocessing_pipeline\n",
    ")\n",
    "from dataruns.core.pipeline import Pipeline, Make_Pipeline\n",
    "\n",
    "print(\"✓ All imports successful!\")\n",
    "print(f\"NumPy version: {np.__version__}\")\n",
    "print(f\"Pandas version: {pd.__version__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6477b034",
   "metadata": {},
   "source": [
    "## Sample Data Creation\n",
    "\n",
    "First, let's create a sample dataset that we'll use throughout all our examples. This dataset includes various types of features and intentionally has missing values to demonstrate data preprocessing techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "707327ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset shape: (100, 5)\n",
      "Missing values per column:\n",
      "feature1    5\n",
      "feature2    5\n",
      "feature3    5\n",
      "feature4    0\n",
      "category    0\n",
      "dtype: int64\n",
      "\n",
      "Dataset info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100 entries, 0 to 99\n",
      "Data columns (total 5 columns):\n",
      " #   Column    Non-Null Count  Dtype  \n",
      "---  ------    --------------  -----  \n",
      " 0   feature1  95 non-null     float64\n",
      " 1   feature2  95 non-null     float64\n",
      " 2   feature3  95 non-null     float64\n",
      " 3   feature4  100 non-null    float64\n",
      " 4   category  100 non-null    object \n",
      "dtypes: float64(4), object(1)\n",
      "memory usage: 4.0+ KB\n",
      "None\n",
      "\n",
      "First 5 rows:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "feature1",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature2",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature3",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "feature4",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "category",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "736728d0-b3fb-4caa-83c8-037e351e2772",
       "rows": [
        [
         "0",
         "10.993428306022466",
         "3.5846292579495858",
         "0.17889368017414164",
         "0.6626677246134771",
         "A"
        ],
        [
         "1",
         "9.723471397657631",
         "4.579354677234641",
         "0.2803922631841172",
         "0.39066659208089394",
         "C"
        ],
        [
         "2",
         "11.295377076201385",
         "4.65728548347323",
         "0.5415256215876385",
         "2.7775127920108496",
         "B"
        ],
        [
         "3",
         "13.046059712816051",
         "4.197722730778381",
         "0.5269010260174515",
         "3.288418366486839",
         "B"
        ],
        [
         "4",
         "9.531693250553328",
         "4.838714288333991",
         "-0.6888346839785455",
         "9.314009780280461",
         "A"
        ]
       ],
       "shape": {
        "columns": 5,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>feature1</th>\n",
       "      <th>feature2</th>\n",
       "      <th>feature3</th>\n",
       "      <th>feature4</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10.993428</td>\n",
       "      <td>3.584629</td>\n",
       "      <td>0.178894</td>\n",
       "      <td>0.662668</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>9.723471</td>\n",
       "      <td>4.579355</td>\n",
       "      <td>0.280392</td>\n",
       "      <td>0.390667</td>\n",
       "      <td>C</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>11.295377</td>\n",
       "      <td>4.657285</td>\n",
       "      <td>0.541526</td>\n",
       "      <td>2.777513</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>13.046060</td>\n",
       "      <td>4.197723</td>\n",
       "      <td>0.526901</td>\n",
       "      <td>3.288418</td>\n",
       "      <td>B</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>9.531693</td>\n",
       "      <td>4.838714</td>\n",
       "      <td>-0.688835</td>\n",
       "      <td>9.314010</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    feature1  feature2  feature3  feature4 category\n",
       "0  10.993428  3.584629  0.178894  0.662668        A\n",
       "1   9.723471  4.579355  0.280392  0.390667        C\n",
       "2  11.295377  4.657285  0.541526  2.777513        B\n",
       "3  13.046060  4.197723  0.526901  3.288418        B\n",
       "4   9.531693  4.838714 -0.688835  9.314010        A"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_sample_data():\n",
    "    \"\"\"Create sample data for demonstration.\"\"\"\n",
    "    np.random.seed(42)\n",
    "    data = pd.DataFrame({\n",
    "        'feature1': np.random.normal(10, 2, 100),      # Normal distribution\n",
    "        'feature2': np.random.normal(5, 1, 100),       # Different scale\n",
    "        'feature3': np.random.normal(0, 0.5, 100),     # Centered around 0\n",
    "        'feature4': np.random.exponential(2, 100),     # Exponential distribution\n",
    "        'category': np.random.choice(['A', 'B', 'C'], 100)  # Categorical\n",
    "    })\n",
    "    \n",
    "    # Add some missing values to simulate real-world data\n",
    "    data.iloc[5:10, 0] = np.nan    # Missing in feature1\n",
    "    data.iloc[15:20, 1] = np.nan   # Missing in feature2\n",
    "    data.iloc[25:30, 2] = np.nan   # Missing in feature3\n",
    "    \n",
    "    return data\n",
    "\n",
    "# Create our sample dataset\n",
    "sample_data = create_sample_data()\n",
    "\n",
    "print(f\"Dataset shape: {sample_data.shape}\")\n",
    "print(f\"Missing values per column:\")\n",
    "print(sample_data.isnull().sum())\n",
    "print(f\"\\nDataset info:\")\n",
    "print(sample_data.info())\n",
    "print(f\"\\nFirst 5 rows:\")\n",
    "sample_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18c01d59",
   "metadata": {},
   "source": [
    "## Example 1: Standalone Transforms\n",
    "\n",
    "In this example, we'll use individual transforms separately. This approach gives you full control over each step of the preprocessing pipeline and is useful when you need to inspect intermediate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00601b75",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXAMPLE 1: Standalone Transforms\n",
      "============================================================\n",
      "Original data shape: (100, 5)\n",
      "Missing values: 15\n",
      "After filling NA: 0 missing values\n",
      "Scaled data mean: [0.0, -0.0, 0.0, 0.0]\n",
      "Scaled data std: [1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "First 5 rows of processed data:\n",
      "   feature1  feature2  feature3  feature4\n",
      "0  0.711342 -1.525980  0.271283 -0.730803\n",
      "1 -0.006724 -0.463299  0.461248 -0.849971\n",
      "2  0.882072 -0.380044  0.949985  0.195742\n",
      "3  1.871952 -0.871003  0.922614  0.419577\n",
      "4 -0.115160 -0.186221 -1.352758  3.059478\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXAMPLE 1: Standalone Transforms\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "data = create_sample_data()\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "print(f\"Missing values: {data.isnull().sum().sum()}\")\n",
    "\n",
    "# Step 1: Handle missing values\n",
    "fill_na = FillNA(method='mean')\n",
    "numerical_data = data.select_dtypes(include=[np.number])\n",
    "data_filled = fill_na.fit_transform(numerical_data)\n",
    "print(f\"After filling NA: {data_filled.isnull().sum().sum()} missing values\")\n",
    "\n",
    "# Step 2: Scale features\n",
    "scaler = StandardScaler()\n",
    "data_scaled = scaler.fit_transform(data_filled)\n",
    "print(f\"Scaled data mean: {data_scaled.mean().round(3).tolist()}\")\n",
    "print(f\"Scaled data std: {data_scaled.std().round(3).tolist()}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of processed data:\")\n",
    "print(data_scaled.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eded269",
   "metadata": {},
   "source": [
    "## Example 2: Transform Composer\n",
    "\n",
    "The `TransformComposer` allows you to chain multiple transforms together in a single, reusable pipeline. This is more convenient than applying transforms individually and ensures consistency across different datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8fa5b1da",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXAMPLE 2: Transform Composer\n",
      "============================================================\n",
      "Processed data shape: (100, 4)\n",
      "Missing values after processing: 0\n",
      "\n",
      "First 5 rows of processed data:\n",
      "   feature1  feature2  feature3  feature4\n",
      "0  0.711677 -1.528818  0.269137 -0.730803\n",
      "1 -0.006388 -0.466226  0.459093 -0.849971\n",
      "2  0.882407 -0.382978  0.947809  0.195742\n",
      "3  1.872286 -0.873895  0.920439  0.419577\n",
      "4 -0.114824 -0.189171 -1.354833  3.059478\n",
      "\n",
      "Data statistics after preprocessing:\n",
      "Mean: [0.0, 0.0, 0.0, 0.0]\n",
      "Std: [1.0, 1.0, 1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXAMPLE 2: Transform Composer\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "data = create_sample_data()\n",
    "\n",
    "# Create a preprocessing pipeline using TransformComposer\n",
    "preprocessor = TransformComposer(\n",
    "    SelectColumns(['feature1', 'feature2', 'feature3', 'feature4']),  # Select numerical columns\n",
    "    FillNA(method='median'),                                          # Fill missing values with median\n",
    "    StandardScaler()                                                  # Standardize features\n",
    ")\n",
    "\n",
    "# Apply the entire pipeline at once\n",
    "result = preprocessor.fit_transform(data)\n",
    "print(f\"Processed data shape: {result.shape}\")\n",
    "print(f\"Missing values after processing: {result.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nFirst 5 rows of processed data:\")\n",
    "print(result.head())\n",
    "\n",
    "print(f\"\\nData statistics after preprocessing:\")\n",
    "print(f\"Mean: {result.mean().round(3).tolist()}\")\n",
    "print(f\"Std: {result.std().round(3).tolist()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e350be68",
   "metadata": {},
   "source": [
    "## Example 3: Integration with Dataruns Pipeline\n",
    "\n",
    "This example shows how to integrate transforms with the main dataruns `Pipeline` class. This approach allows you to combine data preprocessing with other data processing steps in a unified workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "44982c41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXAMPLE 3: Integration with Dataruns Pipeline\n",
      "============================================================\n",
      "Added interaction feature: feature1_x_feature2\n",
      "Pipeline result shape: (100, 5)\n",
      "Pipeline result (first 5 rows):\n",
      "   feature1  feature2  feature3  feature4  feature1_x_feature2\n",
      "0  0.696879  0.108516  0.507338  0.064788             0.075623\n",
      "1  0.554890  0.322946  0.535953  0.037291             0.179199\n",
      "2  0.730639  0.339745  0.609574  0.278584             0.248231\n",
      "3  0.926376  0.240679  0.605451  0.330234             0.222959\n",
      "4  0.533448  0.378855  0.262701  0.939380             0.202099\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXAMPLE 3: Integration with Dataruns Pipeline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "data = create_sample_data()\n",
    "\n",
    "# Define preprocessing function using transforms\n",
    "def preprocess_numerical(df):\n",
    "    \"\"\"Preprocess numerical columns using transforms.\"\"\"\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    preprocessor = TransformComposer(\n",
    "        SelectColumns(numerical_cols.tolist()),\n",
    "        FillNA(method='mean'),\n",
    "        MinMaxScaler(feature_range=(0, 1))\n",
    "    )\n",
    "    return preprocessor.fit_transform(df)\n",
    "\n",
    "def add_feature_engineering(data):\n",
    "    \"\"\"Add some feature engineering.\"\"\"\n",
    "    result = data.copy()\n",
    "    if hasattr(data, 'columns') and len(data.columns) >= 2:\n",
    "        # Add interaction features for dataframes\n",
    "        result[f'{data.columns[0]}_x_{data.columns[1]}'] = data.iloc[:, 0] * data.iloc[:, 1]\n",
    "        print(f\"Added interaction feature: {data.columns[0]}_x_{data.columns[1]}\")\n",
    "    return result\n",
    "\n",
    "# Create dataruns pipeline that combines preprocessing with feature engineering\n",
    "pipeline = Pipeline(\n",
    "    preprocess_numerical,      # Step 1: Preprocess numerical data\n",
    "    add_feature_engineering    # Step 2: Add engineered features\n",
    ")\n",
    "\n",
    "# Apply the entire pipeline\n",
    "result = pipeline(data)\n",
    "print(f\"Pipeline result shape: {result.shape}\")\n",
    "print(\"Pipeline result (first 5 rows):\")\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15b89983",
   "metadata": {},
   "source": [
    "## Example 4: Custom Preprocessing Pipeline\n",
    "\n",
    "The dataruns library provides convenience functions like `create_preprocessing_pipeline()` that create common preprocessing workflows with just a few parameters. This is perfect for rapid prototyping and standard preprocessing tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9f64137a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXAMPLE 4: Custom Preprocessing Pipeline\n",
      "============================================================\n",
      "Original data range:\n",
      "Min: [4.761, 3.081, -1.621, 0.022]\n",
      "Max: [13.705, 7.72, 1.926, 9.914]\n",
      "\n",
      "Processed data range (should be 0-1 after MinMax scaling):\n",
      "Min: [0.0, 0.0, 0.0, 0.0]\n",
      "Max: [1.0, 1.0, 1.0, 1.0]\n",
      "\n",
      "Missing values before: 15\n",
      "Missing values after: 0\n",
      "\n",
      "Processed data (first 5 rows):\n",
      "   feature1  feature2  feature3  feature4\n",
      "0  0.696879  0.108516  0.507338  0.064788\n",
      "1  0.554890  0.322946  0.535953  0.037291\n",
      "2  0.730639  0.339745  0.609574  0.278584\n",
      "3  0.926376  0.240679  0.605451  0.330234\n",
      "4  0.533448  0.378855  0.262701  0.939380\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXAMPLE 4: Custom Preprocessing Pipeline\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "data = create_sample_data()\n",
    "\n",
    "# Create custom preprocessing using the convenience function\n",
    "preprocessing_pipeline = create_preprocessing_pipeline(\n",
    "    scale_method='minmax',    # Use MinMax scaling\n",
    "    handle_missing='fill',    # Fill missing values\n",
    "    fill_value=None          # Use mean for filling (None defaults to mean)\n",
    ")\n",
    "\n",
    "# Apply to numerical data only\n",
    "numerical_data = data.select_dtypes(include=[np.number])\n",
    "processed = preprocessing_pipeline.fit_transform(numerical_data)\n",
    "\n",
    "print(f\"Original data range:\")\n",
    "print(f\"Min: {numerical_data.min().round(3).tolist()}\")\n",
    "print(f\"Max: {numerical_data.max().round(3).tolist()}\")\n",
    "\n",
    "print(f\"\\nProcessed data range (should be 0-1 after MinMax scaling):\")\n",
    "print(f\"Min: {processed.min().round(3).tolist()}\")\n",
    "print(f\"Max: {processed.max().round(3).tolist()}\")\n",
    "\n",
    "print(f\"\\nMissing values before: {numerical_data.isnull().sum().sum()}\")\n",
    "print(f\"Missing values after: {processed.isnull().sum().sum()}\")\n",
    "\n",
    "print(\"\\nProcessed data (first 5 rows):\")\n",
    "print(processed.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82ba5788",
   "metadata": {},
   "source": [
    "## Example 5: Advanced Pipeline with Custom Transforms\n",
    "\n",
    "In this final example, we'll create a sophisticated preprocessing pipeline that includes custom functions for outlier removal and log transformation, combined with standard preprocessing steps. This showcases the flexibility of the dataruns system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b8893e1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EXAMPLE 5: Advanced Pipeline with Custom Transforms\n",
      "============================================================\n",
      "Outlier removal: 100 -> 82 rows (18 outliers removed)\n",
      "Applied log transformation to feature1\n",
      "Applied log transformation to feature2\n",
      "Applied log transformation to feature4\n",
      "Advanced pipeline result shape: (82, 4)\n",
      "Original data shape: (100, 5)\n",
      "Advanced pipeline result (first 5 rows):\n",
      "   feature1  feature2  feature3  feature4\n",
      "0  0.757813 -1.843210  0.272577 -0.693713\n",
      "1  0.001856 -0.351475  0.492754 -0.999505\n",
      "2  0.925752 -0.246100  1.059219  0.711041\n",
      "3  1.824862 -0.889727  1.027495  0.928183\n",
      "5  0.001856  0.695800 -1.132682 -0.323458\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*60)\n",
    "print(\"EXAMPLE 5: Advanced Pipeline with Custom Transforms\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "data = create_sample_data()\n",
    "\n",
    "# Define custom transformation functions\n",
    "def remove_outliers(df, n_std=2):\n",
    "    \"\"\"Remove outliers beyond n standard deviations.\"\"\"\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    original_shape = df.shape[0]\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        mean = df[col].mean()\n",
    "        std = df[col].std()\n",
    "        df = df[(df[col] >= mean - n_std * std) & (df[col] <= mean + n_std * std)]\n",
    "    \n",
    "    print(f\"Outlier removal: {original_shape} -> {df.shape[0]} rows ({original_shape - df.shape[0]} outliers removed)\")\n",
    "    return df\n",
    "\n",
    "def log_transform_positive(df):\n",
    "    \"\"\"Apply log transformation to positive values.\"\"\"\n",
    "    result = df.copy()\n",
    "    numerical_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    \n",
    "    for col in numerical_cols:\n",
    "        if (df[col] > 0).all():\n",
    "            result[col] = np.log1p(df[col])  # log1p is log(1+x), safer for values near 0\n",
    "            print(f\"Applied log transformation to {col}\")\n",
    "    \n",
    "    return result\n",
    "\n",
    "# Create advanced pipeline using Make_Pipeline\n",
    "advanced_pipeline = Make_Pipeline()\n",
    "advanced_pipeline.add(\n",
    "    lambda x: x.select_dtypes(include=[np.number]),           # Select numerical columns\n",
    "    lambda x: FillNA(method='median').fit_transform(x),      # Fill missing values\n",
    "    remove_outliers,                                         # Remove outliers\n",
    "    log_transform_positive,                                  # Log transform\n",
    "    lambda x: StandardScaler().fit_transform(x)             # Standardize\n",
    ")\n",
    "\n",
    "# Build and apply the pipeline\n",
    "pipeline = advanced_pipeline.build()\n",
    "result = pipeline(data)\n",
    "\n",
    "print(f\"Advanced pipeline result shape: {result.shape}\")\n",
    "print(f\"Original data shape: {data.shape}\")\n",
    "print(\"Advanced pipeline result (first 5 rows):\")\n",
    "print(result.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a001b2",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "🎉 **Congratulations!** You've successfully explored all the comprehensive transform examples in the dataruns library.\n",
    "\n",
    "### What We've Covered:\n",
    "\n",
    "1. **Standalone Transforms**: How to use individual transforms step-by-step for fine-grained control\n",
    "2. **Transform Composer**: Chaining multiple transforms into reusable preprocessing pipelines\n",
    "3. **Pipeline Integration**: Combining transforms with the main dataruns Pipeline for complete workflows\n",
    "4. **Convenience Functions**: Using `create_preprocessing_pipeline()` for rapid prototyping\n",
    "5. **Advanced Pipelines**: Building sophisticated preprocessing workflows with custom functions\n",
    "\n",
    "### Key Takeaways:\n",
    "\n",
    "- ✅ **Flexibility**: Multiple approaches for different use cases\n",
    "- ✅ **Reusability**: Create preprocessing pipelines that can be applied to new data\n",
    "- ✅ **Integration**: Seamless integration with the broader dataruns ecosystem\n",
    "- ✅ **Customization**: Easy to add custom preprocessing steps\n",
    "- ✅ **Scalability**: Efficient processing for large datasets\n",
    "\n",
    "### Next Steps:\n",
    "\n",
    "- Try applying these patterns to your own datasets\n",
    "- Experiment with different scaling methods and missing value strategies\n",
    "- Create custom transforms for domain-specific preprocessing needs\n",
    "- Combine with dataruns' other features for complete data processing workflows\n",
    "\n",
    "Happy data processing! 🚀"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1cba62b",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dataruns",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
